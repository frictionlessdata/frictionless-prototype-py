{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transforming-data.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPyVpYJq13FwRBmBvK6Hwu8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F4AUuGTrqzdv","colab_type":"text"},"source":["# Transforming Data\n","\n","> Transform functionality is work-in-progress\n","\n","Transforming data in Frictionless means modifying a data + metadata set from the state A to the state B. For example, it can be a dirty Excel file we need to transform to a cleaned CSV file or a folder of data files we want to update and save as a data package.\n","\n","The most high-level way of transforming data is data pipelines. Frictionless supports two types of data pipelines:\n","- package pipelines\n","- resource pipelines\n","\n","The package pipelines are powered by [DataFlows](http://www.dataflows.org/), \n","a novel and intuitive way of building data processing flows in Python. Frictionless provides an ability to declaratively describe and run DataFlows pipelines.\n","\n","The resource pipelines are under active development and they are not ready to be used yet. We will update this guide when it's ready adding a resource-level documentation and examples.\n","\n","Of course, it's not only possible to use pipelines to transform data in Frictionless; we can also use lower-level primitives like `Table` class to modify the data using plain Python programming. \n","\n"]},{"cell_type":"code","metadata":{"id":"tV11vYlpRfVm","colab_type":"code","colab":{}},"source":["! pip install frictionless[dataflows]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OERd2XLbRqh-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1596524515183,"user_tz":-180,"elapsed":3876,"user":{"displayName":"Evgeny Karev","photoUrl":"","userId":"12160649330696878788"}},"outputId":"d27bbe9f-1738-4a18-eaa4-69c7bccbdc75"},"source":["! wget -q -O capital.csv https://raw.githubusercontent.com/frictionlessdata/frictionless-py/master/data/capital-3.csv\n","! cat capital.csv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["id,name\n","1,London\n","2,Berlin\n","3,Paris\n","4,Madrid\n","5,Rome\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tBuPgpCBlw-6","colab_type":"text"},"source":["## Transform Functions\n","\n","The high-level interface for validating data provided by Frictionless is a set of `transform` functions:\n","- `transform`: it will detect the source type and transform data accordingly\n","- `transform_package`: it transforms a package using a DataFlows pipeline descriptor\n","- `transform_resource`: it transforms a resource (under construction)"]},{"cell_type":"markdown","metadata":{"id":"IOax4VJwHsCp","colab_type":"text"},"source":["### Transforming Package\n","\n","Let's see how we can use a package pipelines to transform data using [DataFlows](http://www.dataflows.org/):"]},{"cell_type":"code","metadata":{"id":"Uc5W-57RUDo9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":252},"executionInfo":{"status":"ok","timestamp":1596525209195,"user_tz":-180,"elapsed":3335,"user":{"displayName":"Evgeny Karev","photoUrl":"","userId":"12160649330696878788"}},"outputId":"e9ad86ab-6cb6-4e62-cb95-b0babadd035c"},"source":["! wget -q -O pipeline.yaml https://raw.githubusercontent.com/frictionlessdata/frictionless-py/master/data/pipeline-docs.yaml\n","! cat pipeline.yaml"],"execution_count":null,"outputs":[{"output_type":"stream","text":["name: pipeline\n","type: package\n","steps:\n","  - type: load\n","    spec:\n","      loadSource: 'capital.csv'\n","  - type: set_type\n","    spec:\n","      name: id\n","      type: string\n","  - type: dump_to_path\n","    spec:\n","      outPath: 'output'\n","      prettyDescriptor: true\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YRhPaqqMUbyW","colab_type":"code","colab":{}},"source":["! frictionless transform pipeline.yaml"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tg8ZrJ9UUkyP","colab_type":"text"},"source":["Of course, it's possible to do the same using Python programming:"]},{"cell_type":"code","metadata":{"id":"3b3jzZo_Rxgx","colab_type":"code","cellView":"code","colab":{}},"source":["from frictionless import transform\n","\n","transform(\n","  {\n","    \"type\": \"package\",\n","    \"steps\": [\n","      {\"type\": \"load\", \"spec\": {\"loadSource\": \"capital.csv\"}},\n","      {\"type\": \"set_type\", \"spec\": {\"name\": \"id\", \"type\": \"string\"}},\n","      {\"type\": \"dump_to_path\", \"spec\": {\"outPath\": 'output'}},\n","    ],\n","  }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Afyl7cHDSdyV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"status":"ok","timestamp":1596525909939,"user_tz":-180,"elapsed":1942,"user":{"displayName":"Evgeny Karev","photoUrl":"","userId":"12160649330696878788"}},"outputId":"f3f26edd-77aa-462c-c0c7-da5ed9800b27"},"source":["! ls -la output"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 16\n","drwxr-xr-x 2 root root 4096 Aug  4 07:04 .\n","drwxr-xr-x 1 root root 4096 Aug  4 07:13 ..\n","-rw------- 1 root root   56 Aug  4 07:25 capital.csv\n","-rw------- 1 root root  937 Aug  4 07:25 datapackage.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iPuOLXoXXDaA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"executionInfo":{"status":"ok","timestamp":1596525937658,"user_tz":-180,"elapsed":2983,"user":{"displayName":"Evgeny Karev","photoUrl":"","userId":"12160649330696878788"}},"outputId":"4f7c58c5-8e64-456d-eb88-059bc756385a"},"source":["! frictionless extract output/datapackage.json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1m[data] output/capital.csv\n","\u001b[0m\n","  id  name\n","----  ------\n","   1  London\n","   2  Berlin\n","   3  Paris\n","   4  Madrid\n","   5  Rome\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2a1_G-F9XRKP","colab_type":"text"},"source":["DataFlows is a powerful framework. Please read more about it:\n","- [DataFlows Tutorial](https://github.com/datahq/dataflows/blob/master/TUTORIAL.md)\n","- [DataFlows Processors](https://github.com/datahq/dataflows/blob/master/PROCESSORS.md)"]},{"cell_type":"markdown","metadata":{"id":"0UJGzBS3HuUJ","colab_type":"text"},"source":["### Transforming Resource\n","\n","This functionality is under construction."]},{"cell_type":"markdown","metadata":{"id":"pNWvkCjTHtNy","colab_type":"text"},"source":["## Transform Options\n","\n","For now, the `transorm` function accepts only the `source` option which can be a pipeline descriptor or a resource descriptor (not implemented yet)."]},{"cell_type":"markdown","metadata":{"id":"ndhlAmIjPcFY","colab_type":"text"},"source":["**Package**\n","\n","The `transform_package` functions don't accept any additional arguments."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ONua_J1mPo82"},"source":["**Resource**\n","\n","The `transform_resource` functions don't accept any additional arguments."]},{"cell_type":"markdown","metadata":{"id":"O_Co929HlrbG","colab_type":"text"},"source":["\n","## Using Table\n","\n","On the lowest-level of the transform capabilities we can just use the Table class for whatever manipulations we are interested in. It's just pure Python so you can re-use all your programming skills here. Also, it's important to mention that the approach below is streaming so we can handle very big files. Let's see on an example:"]},{"cell_type":"code","metadata":{"id":"ms9sO4ENYIvB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596526313843,"user_tz":-180,"elapsed":660,"user":{"displayName":"Evgeny Karev","photoUrl":"","userId":"12160649330696878788"}},"outputId":"519597b3-1fed-454f-c58e-a1ad04ac4487"},"source":["from pprint import pprint\n","from frictionless import Table\n","\n","def source():\n","  with Table('capital.csv') as table:\n","    for row in table:\n","      if not row['id'] % 2:\n","        yield row\n","\n","with Table(source) as table:\n","  pprint(table.read_rows())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Row([('id', 2), ('name', 'Berlin')]), Row([('id', 4), ('name', 'Madrid')])]\n"],"name":"stdout"}]}]}